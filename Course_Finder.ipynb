{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahmancodes/Course-Database/blob/main/Course_Finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "aqUERpyICMGC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBYuAUY1tmqV",
        "outputId": "6b8183f4-9ef8-4ad8-a3cf-0a2e06eb22f5",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ust = pd.read_excel(\"HKUST Comp.xlsx\").dropna()\n",
        "hku = pd.read_csv(\"HKU Comp.csv\")"
      ],
      "metadata": {
        "id": "ij74fUYmCON7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "w1 = ust[\"Description\"][0]\n",
        "w2 = ust[\"Title\"][0]\n",
        "w1 = nlp(w1)\n",
        "w2 = nlp(w2)\n",
        "w2s = [sent.text for sent in w2.sents]\n",
        "w2s\n",
        "for i in range(len(hku)):\n",
        "  y = nlp(hku[\"Description\"][i])\n",
        "  w3ns = nlp(' '.join([str(t) for t in y if not t.is_stop]))\n",
        "  q = w1ns.similarity(w3ns)\n",
        "  print(q)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "gGpBHINNCnOq",
        "outputId": "98b56763-8ba9-4487-840e-c8b5b0cbf736"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nw1 = ust[\"Description\"][0]\\nw2 = ust[\"Title\"][0]\\nw1 = nlp(w1)\\nw2 = nlp(w2)\\nw2s = [sent.text for sent in w2.sents]\\nw2s\\nfor i in range(len(hku)):\\n  y = nlp(hku[\"Description\"][i])\\n  w3ns = nlp(\\' \\'.join([str(t) for t in y if not t.is_stop]))\\n  q = w1ns.similarity(w3ns)\\n  print(q)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-universal-sentence-encoder\n",
        "import spacy_universal_sentence_encoder\n",
        "# load one of the models: ['en_use_md', 'en_use_lg', 'xx_use_md', 'xx_use_lg']\n",
        "nlp = spacy_universal_sentence_encoder.load_model('en_use_lg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ6z3gkPbaqq",
        "outputId": "1eb93df0-a5c8-426b-e9d1-4fb156b35f1e",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy-universal-sentence-encoder\n",
            "  Downloading spacy_universal_sentence_encoder-0.4.6.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-universal-sentence-encoder) (2.15.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy-universal-sentence-encoder) (3.7.5)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (from spacy-universal-sentence-encoder) (0.16.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.25.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub->spacy-universal-sentence-encoder) (2.15.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.43.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2024.6.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.1.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.3.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-universal-sentence-encoder) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.2.2)\n",
            "Building wheels for collected packages: spacy-universal-sentence-encoder\n",
            "  Building wheel for spacy-universal-sentence-encoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-universal-sentence-encoder: filename=spacy_universal_sentence_encoder-0.4.6-py3-none-any.whl size=16539 sha256=d6047490b7b1b273dba6725bc8ce4e91c3e5804779282712d6f238699813b0f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/75/56/7fd780bb5e1f9c74625ccda3c03a1abeb27a2ade5a8946db33\n",
            "Successfully built spacy-universal-sentence-encoder\n",
            "Installing collected packages: spacy-universal-sentence-encoder\n",
            "Successfully installed spacy-universal-sentence-encoder-0.4.6\n",
            "Downloaded https://tfhub.dev/google/universal-sentence-encoder-large/5, Total size: 577.10MB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "courses = {0:[0,1],1:[0],2:[2],3:[0],4:[0,1],5:[],6:[17],7:[1],8:[9],9:[9],10:[26],11:[7],12:[38],13:[8],14:[8],15:[],16:[],17:[25],18:[0],19:[16],20:[16],21:[]}"
      ],
      "metadata": {
        "id": "39MMaa-p4DNI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = ust[\"Description\"][13]\n",
        "w2 = ust[\"Title\"][13]\n",
        "w1 = nlp(w1)\n",
        "w2 = nlp(w2)\n",
        "w3 = hku[\"Title\"][8]\n",
        "w3 = nlp(w3)"
      ],
      "metadata": {
        "id": "YxixNFVwCzq-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''for i in range(len(hku)):\n",
        "  y = nlp(hku[\"Title\"][i])\n",
        "  q = w2.similarity(y)\n",
        "  print(i,q)\n",
        "'''\n",
        "correct = incorrect = max_score = mappingNA = 0\n",
        "documents = [hku[\"Description\"][i] for i in range(len(hku))] #A list with all the Descriptions of HKU Courses\n",
        "for i in range(len(ust)):\n",
        "  documents.insert(0,ust[\"Description\"][i]) #Appending  UST Description to the same list, at Index 0, one by one\n",
        "  w1 = nlp(ust[\"Description\"][i])\n",
        "  for j in range(len(hku)):\n",
        "    w2 = nlp(hku[\"Description\"][j])\n",
        "    sim_score = w1.similarity(w2)\n",
        "    if sim_score > max_score:\n",
        "        max_score = sim_score\n",
        "        best_sim_index = j-1\n",
        "\n",
        "  found= 2\n",
        "  if i in courses:\n",
        "\n",
        "    for numberofmapping in range(len(courses[i])):\n",
        "      if courses[i][numberofmapping] == best_sim_index:\n",
        "        found=1\n",
        "      else:\n",
        "        found=0\n",
        "\n",
        "    if (found==1):\n",
        "      correct=correct+1\n",
        "    if (found==0):\n",
        "      incorrect=incorrect+1\n",
        "    if (found==2):\n",
        "      mappingNA=mappingNA+1\n",
        "\n",
        "  del documents[0]\n",
        "accuracy = correct/(correct+incorrect)\n",
        "accuracy,correct,incorrect,mappingNA\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G73LgS_Dq6g",
        "outputId": "5e1c6cd7-5a68-4657-b048-bc8bbc578292"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16666666666666666, 3, 15, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sklearn"
      ],
      "metadata": {
        "id": "2QrrdpWvD0aj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "'''\n",
        "# Compute cosine similarity\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print the similarity matrix\n",
        "print(\"Cosine similarity matrix:\")\n",
        "all_course = np.delete(cosine_sim[0],0)\n",
        "\n",
        "best_sim_index = all_course.argmax()'''\n",
        "correct = incorrect = mappingNA = 0\n",
        "documents = [hku[\"Description\"][i] for i in range(len(hku))]\n",
        "for i in range(len(ust)):\n",
        "  documents.insert(0,ust[\"Description\"][i])\n",
        "  # Create a TF-IDF vectorizer\n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # Fit and transform the documents\n",
        "  tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "  # Compute cosine similarity\n",
        "  cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "  all_course = np.delete(cosine_sim[0],0)\n",
        "\n",
        "  best_sim_index = all_course.argmax()\n",
        "\n",
        "  found= 2\n",
        "  if i in courses:\n",
        "\n",
        "    for numberofmapping in range(len(courses[i])):\n",
        "      if courses[i][numberofmapping] == best_sim_index:\n",
        "        found=1\n",
        "      else:\n",
        "        found=0\n",
        "\n",
        "    if (found==1):\n",
        "      correct=correct+1\n",
        "    if (found==0):\n",
        "      incorrect=incorrect+1\n",
        "    if (found==2):\n",
        "      mappingNA=mappingNA+1\n",
        "\n",
        "  del documents[0]\n",
        "accuracy = correct/(correct+incorrect)\n",
        "accuracy,correct,incorrect,mappingNA"
      ],
      "metadata": {
        "id": "YHBjOzjLJYeJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba26b08-52df-43a5-c27e-32f20f52c91c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3333333333333333, 6, 12, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def compute_similarity(hku, ust, courses):\n",
        "    correct = incorrect = mappingNA = 0\n",
        "    documents = [hku[\"Description\"][i] for i in range(len(hku))]\n",
        "\n",
        "    for i in range(len(ust)):\n",
        "        documents.insert(0, ust[\"Description\"][i])\n",
        "\n",
        "        # Create a TF-IDF vectorizer\n",
        "        vectorizer = TfidfVectorizer()\n",
        "\n",
        "        # Fit and transform the documents\n",
        "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "        all_course = np.delete(cosine_sim[0], 0)\n",
        "\n",
        "        best_sim_index = all_course.argmax()\n",
        "\n",
        "        found = 2\n",
        "        if i in courses:\n",
        "            for numberofmapping in range(len(courses[i])):\n",
        "                if courses[i][numberofmapping] == best_sim_index:\n",
        "                    found = 1\n",
        "                else:\n",
        "                    found = 0\n",
        "\n",
        "            if found == 1:\n",
        "                correct += 1\n",
        "            elif found == 0:\n",
        "                incorrect += 1\n",
        "            elif found == 2:\n",
        "                mappingNA += 1\n",
        "\n",
        "        del documents[0]\n",
        "\n",
        "    accuracy = correct / (correct + incorrect)\n",
        "    return accuracy, correct, incorrect, mappingNA\n",
        "'''"
      ],
      "metadata": {
        "id": "iDATA4VfLTz-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "0496b20c-6bf8-4889-8790-c6dc582275a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nimport numpy as np\\n\\ndef compute_similarity(hku, ust, courses):\\n    correct = incorrect = mappingNA = 0\\n    documents = [hku[\"Description\"][i] for i in range(len(hku))]\\n\\n    for i in range(len(ust)):\\n        documents.insert(0, ust[\"Description\"][i])\\n\\n        # Create a TF-IDF vectorizer\\n        vectorizer = TfidfVectorizer()\\n\\n        # Fit and transform the documents\\n        tfidf_matrix = vectorizer.fit_transform(documents)\\n\\n        # Compute cosine similarity\\n        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\\n\\n        all_course = np.delete(cosine_sim[0], 0)\\n\\n        best_sim_index = all_course.argmax()\\n\\n        found = 2\\n        if i in courses:\\n            for numberofmapping in range(len(courses[i])):\\n                if courses[i][numberofmapping] == best_sim_index:\\n                    found = 1\\n                else:\\n                    found = 0\\n\\n            if found == 1:\\n                correct += 1\\n            elif found == 0:\\n                incorrect += 1\\n            elif found == 2:\\n                mappingNA += 1\\n\\n        del documents[0]\\n\\n    accuracy = correct / (correct + incorrect)\\n    return accuracy, correct, incorrect, mappingNA\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_similarity(hku, ust, courses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "T4ARtnUCiNgW",
        "outputId": "a7dd4f5d-1dcc-4f7a-91d6-7ee20a216937"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'compute_similarity' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e553709bf9d6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhku\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0must\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcourses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'compute_similarity' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "#def compute_similarity(hku, ust, courses):\n",
        "correct = incorrect = mappingNA = user_input = 0\n",
        "asked_courses = {}\n",
        "documents = [hku[\"Description\"][i] for i in range(len(hku))]\n",
        "on_off = np.zeros(len(hku))\n",
        "while user_input != \"stop\":\n",
        "  user_input = input(\"Enter course code\")\n",
        "  if user_input == \"stop\" or user_input not in ust[\"Code\"].values:\n",
        "    break\n",
        "  if user_input not in asked_courses:\n",
        "\n",
        "    # inserts input course description to documents list\n",
        "    documents.insert(0, ust.loc[ust['Code'] == user_input, 'Description'].iloc[0])\n",
        "\n",
        "    # Create a TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit and transform the documents\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    #deletes input course from documents list\n",
        "    best_sim_score = np.delete(cosine_sim[0], 0)\n",
        "    asked_courses[user_input] = best_sim_score\n",
        "\n",
        "  #Finding the index of the most similar course\n",
        "  best_sim_score_index = best_sim_score[0].argmax()\n",
        "  print(hku.loc[best_sim_score_index, \"Code\"])\n",
        "\n",
        "  #Asking for user feedback and updating the model accordingly\n",
        "  user_input_2 = input(\"Is the course matching correct?\")\n",
        "  if user_input_2 == \"yes\":\n",
        "    print(type(best_sim_score))\n",
        "    #We can increase the similarity score of matched course to strengthen the program prediction\n",
        "    print(\"hi\")\n",
        "  elif user_input_2 == \"no\":\n",
        "    #Updating the model slowly for better predictions\n",
        "    #Adding ones to incorrectly predicted course\n",
        "\n",
        "    on_off[best_sim_score_index] = 1\n",
        "    #Appending array to similarity array\n",
        "    if best_sim_score.shape[0]==2:\n",
        "      best_sim_score = np.array([best_sim_score[0],on_off])\n",
        "    else:\n",
        "      best_sim_score = np.array([best_sim_score,on_off])\n",
        "\n",
        "    # Find the indices where the value in the second row is 1\n",
        "    indices = np.where(best_sim_score[1] == 0)[0]\n",
        "    print(best_sim_score,best_sim_score_index)\n",
        "    # Multiply the values in the first row by 2 at the corresponding indices\n",
        "    best_sim_score[0, indices] *= 1.1\n",
        "'''\n",
        "    best_sim_score = np.array([besgt_sim_score,on_off])\n",
        "    best_sim_score[0]*1.1 if best_sim_score[]\n",
        "    best_sim_score[best_sim_score != best_sim_score[best_sim_score_index]] *= 1.10\n",
        "'''\n",
        "\n",
        "\n",
        "asked_courses\n",
        "hku.loc[best_sim_score_index, \"Code\"]\n",
        "      #globals()[user_input+\"sim_score\"] = np.delete(cosine_sim[0], 0)\n",
        "\n",
        "      #globals()[user_input+\"sim_score\"].argmax()\n"
      ],
      "metadata": {
        "id": "6oVxEIEFDYhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d2ddd2-f3e9-4211-d7a8-62c02769b0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter course codeCOMP 1021\n",
            "COMP1117\n",
            "Is the course matching correct?no\n",
            "[[0.19310837 0.18952149 0.22301935 0.11400961 0.21478104 0.22301935\n",
            "  0.01405622 0.16426892 0.0884518  0.12995618 0.09887518 0.01710477\n",
            "  0.03877105 0.24982923 0.16047723 0.0691727  0.12870533 0.21695475\n",
            "  0.14145953 0.10874222 0.12360356 0.15528918 0.21478104 0.14455283\n",
            "  0.24548355 0.14034335 0.04753333 0.07529198 0.10582215 0.15180481\n",
            "  0.09386971 0.24990803 0.12648983 0.29723635 0.18639266 0.0765238\n",
            "  0.         0.15266872 0.08273984 0.20454527 0.11564591 0.07634939\n",
            "  0.01062401 0.1699267  0.1942388  0.15789675]\n",
            " [1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]] 0\n",
            "Enter course codeCOMP 1021\n",
            "COMP3329\n",
            "Is the course matching correct?no\n",
            "[[0.19310837 0.28428223 0.33452903 0.17101441 0.32217156 0.33452903\n",
            "  0.02108434 0.24640338 0.1326777  0.19493427 0.14831277 0.02565715\n",
            "  0.05815658 0.37474384 0.24071585 0.10375905 0.193058   0.32543213\n",
            "  0.21218929 0.16311333 0.18540534 0.23293377 0.32217156 0.21682924\n",
            "  0.36822532 0.21051502 0.07129999 0.11293797 0.15873322 0.22770722\n",
            "  0.14080456 0.37486205 0.18973475 0.44585452 0.279589   0.1147857\n",
            "  0.         0.22900308 0.12410976 0.3068179  0.17346886 0.11452409\n",
            "  0.01593601 0.25489005 0.2913582  0.23684513]\n",
            " [1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         1.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]] 33\n",
            "Enter course codeCOMP 1021\n",
            "COMP3322\n",
            "Is the course matching correct?no\n",
            "[[0.19310837 0.42642334 0.50179355 0.25652162 0.48325734 0.50179355\n",
            "  0.03162651 0.36960506 0.19901655 0.29240141 0.22246916 0.03848573\n",
            "  0.08723486 0.56211576 0.36107378 0.15563857 0.289587   0.4881482\n",
            "  0.31828394 0.24466999 0.27810801 0.34940066 0.48325734 0.32524387\n",
            "  0.55233799 0.31577254 0.10694999 0.16940695 0.23809983 0.34156083\n",
            "  0.21120684 0.56229307 0.28460212 0.44585452 0.41938349 0.17217855\n",
            "  0.         0.34350462 0.18616465 0.46022686 0.26020329 0.17178614\n",
            "  0.02390401 0.38233507 0.4370373  0.3552677 ]\n",
            " [1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         1.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]] 31\n",
            "Enter course codeCOMP 1021\n",
            "COMP3251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "hi1 = np.zeros((4,2))\n",
        "print(hi1.shape)\n"
      ],
      "metadata": {
        "id": "QIKRTGy3JEMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b18431-f9cb-4760-f91d-4414d6652e1b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([[2, 3, 4, 5],\n",
        "                 [1, 0, 1, 1]])\n",
        "\n",
        "# Find the indices where the value in the second row is 1\n",
        "indices = np.where(data[1] == 1)[0]\n",
        "\n",
        "# Multiply the values in the first row by 2 at the corresponding indices\n",
        "data[0, indices] *= 2\n",
        "\n",
        "indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUeqshvkSerO",
        "outputId": "c1ff5a2d-1965-4132-e6f6-71d4b9b3a400"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = {}\n",
        "while user_choice != \"stop\":\n",
        "  user_choice = input(\"Enter course code\")\n",
        "  user_input = int(input(\"Do you think this is the correct course matching? 1 for yes and 0 for no \"))\n",
        "  if user_choice not in feedback:\n",
        "    feedback[user_choice] = []\n",
        "  feedback[user_choice].append(user_input)"
      ],
      "metadata": {
        "id": "yOkrdErb5rLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = {}\n",
        "user_choice = \"\"\n",
        "while user_choice != \"stop\":\n",
        "  user_choice = input(\"Enter course code\")\n",
        "  user_input = int(input(\"Do you think this is the correct course matching? 1 for yes and 0 for no \"))\n",
        "  if user_choice not in feedback:\n",
        "    feedback[user_choice] = []\n",
        "  feedback[user_choice].append(user_input)\n"
      ],
      "metadata": {
        "id": "Dg_P69e154gD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}